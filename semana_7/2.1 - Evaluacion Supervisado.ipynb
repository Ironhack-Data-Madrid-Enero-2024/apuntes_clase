{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 - Evaluación Supervisado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor as RFR    # el modelo , el alias es cosa mia\n",
    "\n",
    "from sklearn.model_selection import train_test_split as tts  # el alias es cosa mia\n",
    "\n",
    "from sklearn.datasets import load_diabetes   # dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_diabetes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['DESCR'].split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data['data'], columns = data['feature_names'])\n",
    "\n",
    "df['target'] = data['target']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = tts(data['data'], data['target'], train_size=0.75)\n",
    "\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelo \n",
    "\n",
    "rfr = RFR()    # inicia el objeto random forest\n",
    "\n",
    "rfr.fit(X_train, y_train)      # entrena el modelo\n",
    " \n",
    "y_pred = rfr.predict(X_test)   # prediccion con el tamaño del test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape, y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.mean(), y_test.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### MSE\n",
    "\n",
    "\n",
    "$$MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^{2}$$\n",
    "\n",
    "\n",
    "pertenece al intervalo [0, +$\\infty$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error as mse    # alias mio\n",
    "\n",
    "\n",
    "mse(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### RMSE\n",
    "\n",
    "\n",
    "$$RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^{2}}$$\n",
    "\n",
    "\n",
    "pertenece al intervalo [0, +$\\infty$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse(y_test, y_pred, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse(y_test, y_pred) ** 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### RMSLE\n",
    "\n",
    "\n",
    "$$RMSLE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(log(y_i)-log(\\hat{y}_i))^{2}}$$\n",
    "\n",
    "\n",
    "pertenece al intervalo [0, +$\\infty$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_log_error as msle\n",
    "\n",
    "msle(y_test, y_pred, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### MAE\n",
    "\n",
    "\n",
    "$$MAE = \\frac{1}{n}\\sum_{i=1}^{n}|y_i-\\hat{y}_i|$$\n",
    "\n",
    "\n",
    "pertenece al intervalo [0, +$\\infty$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "\n",
    "mae(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$MAE <= RMSE <= MAE · \\sqrt{N}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### R2\n",
    "\n",
    "\n",
    "$$R2 = 1 - \\frac{\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^{2}}{\\sum_{i=1}^{n}(y_i-\\bar{y})^{2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Adjusted R2\n",
    "\n",
    "$$AdjustedR2 = 1-(1-R2)\\frac{n-1}{n-p-1}$$\n",
    "\n",
    "\n",
    "donde:\n",
    "+ n = tamaño de la muestra\n",
    "+ p = nº de variables del modelo\n",
    "\n",
    "\n",
    "pertenecen al intervalo (-$\\infty$, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr.score(X_test, y_test)    # R2, calcula internamente la prediccion, viene del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score as r2\n",
    "\n",
    "r2(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_ajustado(r_2):\n",
    "    \n",
    "    adj = 1 - (1-r_2) * (X_test.shape[0]-1) / (X_test.shape[0]-X_test.shape[1]-1)\n",
    "    \n",
    "    return adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_ajustado(r2(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('../data/churn.csv')\n",
    "\n",
    "data=data.dropna()\n",
    "\n",
    "y=data.Churn.apply(lambda x: 1 if x=='Yes' else 0)\n",
    "\n",
    "data=data.drop(columns=['customerID', 'ChurnBinary', 'Churn'])\n",
    "\n",
    "data=pd.get_dummies(data)\n",
    "\n",
    "X=data.copy()\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = tts(X, y, stratify=y)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelo 1 \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression as LogReg\n",
    "\n",
    "logreg = LogReg()\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred_logreg = logreg.predict(X_test)    # devuelve la clase, la etiqueta 0-1\n",
    "\n",
    "y_prob = logreg.predict_proba(X_test)     # devuelve probabilidades\n",
    "\n",
    "y_pred_logreg[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelo 2\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "\n",
    "\n",
    "rfc = RFC()\n",
    "\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rfc = rfc.predict(X_test)\n",
    "\n",
    "\n",
    "y_pred_rfc[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ TP := True Positive (aciertos clase 1)\n",
    "+ TN := True Negative (aciertos clase 0)\n",
    "+ FP := False Positive (Error tipo I, decir 1 cuando es 0)\n",
    "+ FN := False Negative (Error tipo II, decir 0 cuando es 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Accuracy  := (TP+TN)/(TP+TN+FP+FN) (acierto)  ($\\frac{1}{n}\\sum 1(\\hat{y_i}=y_i$))\n",
    "+ Precision := TP/(TP+FP)\n",
    "+ Recall    := TP/(TP+FN)  (Sensibilidad, TPR)\n",
    "+ F1_Score  := 2·Recall·Precision/(Recall+Precision)\n",
    "\n",
    "(F1 funciona mejor que el accuracy cuando los datos no están balanceados y cuando FP y FN son muy diferentes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![f1](images/f1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.score(X_test, y_test)  # accuracy, acierto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score as acc\n",
    "\n",
    "acc(y_test, y_pred_logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc(y_test, y_pred_rfc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score as prec\n",
    "\n",
    "prec(y_test, y_pred_logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec(y_test, y_pred_rfc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score as rec\n",
    "\n",
    "rec(y_test, y_pred_logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec(y_test, y_pred_rfc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### F1_Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score as f1\n",
    "\n",
    "f1(y_test, y_pred_logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1(y_test, y_pred_rfc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Matriz de Confusión\n",
    "\n",
    "![conf_matrix](images/conf_matrix.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix as cm\n",
    "\n",
    "\n",
    "cm(y_test, y_pred_logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "ax=sns.heatmap(cm(y_test, y_pred_rfc)/cm(y_test, y_pred_rfc).sum(), annot=True)\n",
    "\n",
    "plt.title('Matriz confusion')\n",
    "plt.ylabel('Verdad')\n",
    "plt.xlabel('Prediccion')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ROC-AUC  (Característica operativa del receptor y área debajo de la curva)\n",
    "\n",
    "+ TPR := TP/(TP+FN)\n",
    "+ FPR := FP/(TN+FP)\n",
    "\n",
    "\n",
    "![roc](images/roc.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve as roc\n",
    "\n",
    "from sklearn.metrics import roc_auc_score as auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "fpr, tpr, _ = roc(y_test, y_pred_logreg)   # por esto esta mal pintao\n",
    "\n",
    "a=auc(y_test, y_pred_logreg)\n",
    "\n",
    "plt.plot(fpr, tpr)\n",
    "\n",
    "plt.plot(fpr, fpr, 'r--')\n",
    "\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "\n",
    "plt.title('Binary ROC CURVE - AUC={:.3f}'.format(a))\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = logreg.predict_proba(X_test)[::, 1]\n",
    "\n",
    "y_prob[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.predict_proba(X_test)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# BIEN PINTAO, con probabilidad\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "fpr, tpr, _ = roc(y_test, y_prob)   \n",
    "\n",
    "a=auc(y_test, y_prob)\n",
    "\n",
    "plt.plot(fpr, tpr)\n",
    "\n",
    "plt.plot(fpr, fpr, 'r--')\n",
    "\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "\n",
    "plt.title('Binary ROC CURVE - AUC={:.3f}'.format(a))\n",
    "\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Probabilidad de que tu clasificador sea mejor que un clasificador random - Kappa de Cohen**\n",
    "\n",
    "https://es.wikipedia.org/wiki/Coeficiente_kappa_de_Cohen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score as kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa(y_test, y_pred_logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiclase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "\n",
    "X=load_wine().data\n",
    "\n",
    "y=load_wine().target\n",
    "\n",
    "X_train, X_test, y_train, y_test = tts(X, y)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_wine()['DESCR'].split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc=RFC()\n",
    "\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "y_pred=rfc.predict(X_test)\n",
    "\n",
    "y_prob_rfc=rfc.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc=SVC(probability=True)\n",
    "\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "y_prob=svc.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikitplot as skplt\n",
    "\n",
    "skplt.metrics.plot_roc(y_test, y_prob_rfc, figsize=(15, 8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skplt.metrics.plot_roc(y_test, y_prob, figsize=(15, 8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "finalized": {
   "timestamp": 1624209106557,
   "trusted": false
  },
  "kernelspec": {
   "display_name": "clase",
   "language": "python",
   "name": "clase"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
